# Nala AI - Ollama + DeepSeek R1 Deployment Checklist

## ðŸŽ¯ Pre-Deployment Summary

The complete Ollama + DeepSeek R1 integration is ready for deployment. Here's what's been implemented:

### âœ… Completed Integration
- **Server Integration**: Added Ollama functions to server.js with complete fallback chain
- **Environment Setup**: Updated .env with Ollama configuration variables
- **Docker Container**: Complete Ollama + DeepSeek R1 container ready for RunPod
- **API Wrapper**: FastAPI music generation service optimized for Strudel.js
- **Testing Suite**: Comprehensive testing scripts and validation tools
- **Health Monitoring**: Updated health endpoint to show Ollama status

### ðŸŽµ Enhanced Fallback Chain
1. **OpenAI GPT-4** (if enabled and configured)
2. **RunPod Ollama DeepSeek R1** (new primary option)
3. **Direct Ollama** (backup endpoint)
4. **Original RunPod API** (legacy fallback)
5. **Local AI Systems** (Phase 1-3 generators)

## ðŸš€ Deployment Steps

### Step 1: Container Registry Setup (5 minutes)

**Prerequisites:**
- Docker Desktop installed and running
- Docker Hub account (or preferred registry)

**Commands:**
```bash
# Navigate to the Ollama directory
cd "/Users/kentino/Not a Label/not-a-label-terminal/runpod-ollama"

# Build the container
docker build -t your-dockerhub-username/nala-ollama:v1.0 .

# Test locally (optional)
docker run -p 11434:11434 -p 8000:8000 your-dockerhub-username/nala-ollama:v1.0

# Push to registry
docker push your-dockerhub-username/nala-ollama:v1.0
```

### Step 2: RunPod Template Creation (10 minutes)

**Go to RunPod Console:**
1. Navigate to **Templates** â†’ **New Template**

**Template Configuration:**
```yaml
Name: Nala AI Ollama DeepSeek R1
Container Image: your-dockerhub-username/nala-ollama:v1.0
Container Disk: 25 GB
Volume Disk: 50 GB (for model storage)
Volume Mount Path: /root/.ollama

Environment Variables:
  OLLAMA_HOST: 0.0.0.0:11434
  OLLAMA_MODEL: deepseek-r1:8b
  API_PORT: 8000
  PYTHONUNBUFFERED: 1

HTTP Ports:
  - 8000 (Music API)
  - 11434 (Ollama API)

GPU Requirements:
  Minimum: RTX 4090 (24GB VRAM)
  Recommended: RTX 6000 Ada (48GB VRAM)
```

### Step 3: Serverless Endpoint Deployment (15 minutes)

**Create Serverless Endpoint:**
1. Go to **Serverless** â†’ **New Endpoint**
2. Select your Nala AI template

**Endpoint Configuration:**
```yaml
Name: nala-ollama-music-ai
Template: Nala AI Ollama DeepSeek R1
Min Workers: 0
Max Workers: 3
Idle Timeout: 5 minutes
Max Job Runtime: 10 minutes
GPU Type: RTX 4090

Scaling Settings:
  Scale Up Threshold: 1 request
  Scale Down Threshold: 0 requests
```

**Copy the endpoint URL** - it will look like:
`https://api.runpod.ai/v2/YOUR_NEW_ENDPOINT_ID/run`

### Step 4: Production Environment Update (5 minutes)

**Update .env file:**
```bash
# Enable Ollama integration
RUNPOD_OLLAMA_ENDPOINT=https://api.runpod.ai/v2/YOUR_NEW_ENDPOINT_ID/run
RUNPOD_OLLAMA_ENABLED=true

# Ensure you have your RunPod API key
RUNPOD_API_KEY=your_actual_runpod_api_key_here

# Optional: Set direct endpoint for testing
OLLAMA_DIRECT_ENDPOINT=https://your-pod-id-8000.proxy.runpod.net

# Performance settings
API_TIMEOUT=300000
DEEPSEEK_MODEL=deepseek-r1:8b
```

### Step 5: Server Restart and Validation (5 minutes)

**Restart the production server:**
```bash
cd "/Users/kentino/Not a Label/not-a-label-terminal"

# If using PM2
pm2 restart all

# If using nodemon
npm run dev

# If running directly
npm start
```

**Validate the integration:**
```bash
# Check health status
curl -s http://localhost:3001/api/health | jq '.ollama'

# Test music generation
curl -X POST http://localhost:3001/api/generate-music \
  -H "Content-Type: application/json" \
  -d '{
    "userInput": "create a dark trap beat with heavy 808s",
    "musicDNA": {
      "primaryGenre": "trap",
      "preferredMood": "dark",
      "energyLevel": 9
    }
  }' | jq '.metadata.source'
```

### Step 6: Frontend Testing (10 minutes)

**Test from the web interface:**
1. Go to https://not-a-label.art
2. Try generating music with various prompts
3. Check the browser console for any errors
4. Verify the "source" metadata shows Ollama when working

**Expected behavior:**
- If Ollama is available: source = "nala_ollama_deepseek_r1_runpod"
- If fallback is used: source = "nala_phase2_v2" or "nala_phase3_v3"

## ðŸ”§ Troubleshooting Guide

### Issue: Container Won't Start
**Check:**
- RunPod pod logs for startup errors
- GPU memory availability (need 16GB+ for 8B model)
- Container registry credentials

**Solution:**
```bash
# Try smaller model temporarily
Environment Variable: OLLAMA_MODEL=deepseek-r1:1.5b
```

### Issue: Slow Response Times
**Check:**
- GPU type (RTX 4090 minimum recommended)
- Model size vs available memory
- Cold start times (first request takes longer)

**Solution:**
```bash
# Keep containers warm
Min Workers: 1
Idle Timeout: 10 minutes
```

### Issue: High Costs
**Monitor:**
- GPU-hour usage in RunPod dashboard
- Request frequency and patterns
- Idle time settings

**Optimize:**
```bash
# Cost optimization settings
Min Workers: 0
Idle Timeout: 3 minutes
GPU Type: RTX 4090 (vs more expensive options)
```

### Issue: Quality Concerns
**Test:**
- Compare outputs with Phase 2/3 fallbacks
- Adjust temperature in music_api.py
- Try larger model if budget allows

**Improve:**
```bash
# Quality settings
DEEPSEEK_MODEL=deepseek-r1:32b  # If budget allows
Temperature: 0.7-0.9 range
```

## ðŸ“Š Monitoring Dashboard

### Key Metrics to Track
- **Success Rate**: Should be >95%
- **Response Time**: Target <10 seconds
- **Cost per Request**: Track daily/weekly spending
- **Fallback Usage**: Monitor which systems are being used

### Health Check Commands
```bash
# Overall system health
curl -s http://localhost:3001/api/health | jq '.integration_status'

# Test Ollama specifically
curl -s http://localhost:3001/api/generate-music \
  -H "Content-Type: application/json" \
  -d '{"userInput": "test ollama", "musicDNA": {"primaryGenre": "test"}}' \
  | jq '.metadata.source'

# RunPod endpoint health
curl -s https://api.runpod.ai/v2/YOUR_ENDPOINT_ID/health
```

## ðŸŽ‰ Success Criteria

### Technical Success âœ…
- [ ] Ollama container builds and deploys successfully
- [ ] RunPod endpoint responds within 10 seconds
- [ ] Fallback chain works properly
- [ ] Health endpoints show "ollama_ready": true
- [ ] Music generation quality meets expectations

### Business Success âœ…
- [ ] No more authentication failures
- [ ] Improved response consistency
- [ ] Cost reduction vs current setup (if high usage)
- [ ] Enhanced music generation capabilities

### Operational Success âœ…
- [ ] Easy monitoring and debugging
- [ ] Clear error messages and logging
- [ ] Simple rollback if needed
- [ ] Documentation updated

## ðŸ”„ Rollback Plan

If issues arise, you can easily rollback:

### Immediate Rollback (1 minute)
```bash
# Disable Ollama integration
RUNPOD_OLLAMA_ENABLED=false

# Restart server
pm2 restart all
```

### System will automatically fall back to:
1. OpenAI (if configured)
2. Original RunPod API (if available)
3. Local AI systems (Phase 1-3)

## ðŸ“ˆ Next Steps After Deployment

### Week 1: Monitor and Optimize
- Track response times and success rates
- Monitor costs vs previous setup
- Collect user feedback on music quality
- Fine-tune timeout and scaling settings

### Week 2: Performance Tuning
- Optimize model selection based on usage patterns
- Implement request batching if beneficial
- Consider multiple model sizes for different use cases
- Update prompting strategies based on results

### Week 3: Scale and Enhance
- Consider auto-scaling based on traffic patterns
- Implement advanced monitoring and alerting
- Explore larger models for premium features
- Document lessons learned and best practices

---

## ðŸš¨ Important Notes

- **API Keys**: Ensure your RunPod API key is properly configured
- **Budget**: Set spending limits in RunPod dashboard
- **Monitoring**: Check logs regularly during first week
- **Fallback**: The system will gracefully fall back if Ollama fails
- **Updates**: Keep Docker container updated for security

**Ready for deployment!** ðŸš€

All components are tested and integrated. The system will provide better reliability and eliminate the authentication issues you were experiencing with the managed RunPod API.