/*
 * Nala AI - Server Integration for Ollama + DeepSeek R1 on RunPod
 * Add this integration to your existing server.js file
 */

// Configuration
const RUNPOD_OLLAMA_ENDPOINT = process.env.RUNPOD_OLLAMA_ENDPOINT;
const RUNPOD_OLLAMA_ENABLED = process.env.RUNPOD_OLLAMA_ENABLED === 'true';
const OLLAMA_DIRECT_ENDPOINT = process.env.OLLAMA_DIRECT_ENDPOINT;
const API_TIMEOUT = parseInt(process.env.API_TIMEOUT) || 300000; // 5 minutes

// New Ollama-based RunPod API Integration
async function callRunPodOllamaAPI(userInput, musicDNA, context) {
  const prompt = createMusicGenerationPrompt(userInput, musicDNA, context);
  
  console.log('ðŸ¤– Calling RunPod Ollama with prompt:', prompt.substring(0, 100) + '...');
  
  const response = await fetch(RUNPOD_OLLAMA_ENDPOINT, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${RUNPOD_API_KEY}`
    },
    body: JSON.stringify({
      input: {
        userInput: userInput,
        musicDNA: musicDNA,
        context: context,
        max_tokens: 1000,
        temperature: 0.8
      }
    })
  });

  if (!response.ok) {
    throw new Error(`RunPod Ollama API error: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  console.log('ðŸ¤– RunPod Ollama response:', data);

  // Parse the response from our custom handler
  return parseOllamaResponse(data, userInput, musicDNA);
}

// Alternative: Direct Ollama API call (if running your own endpoint)
async function callDirectOllamaAPI(userInput, musicDNA, context) {
  const OLLAMA_ENDPOINT = process.env.OLLAMA_ENDPOINT || 'https://your-ollama-instance.runpod.io';
  
  console.log('ðŸ¤– Calling direct Ollama endpoint...');
  
  const response = await fetch(`${OLLAMA_ENDPOINT}/api/generate-music`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${process.env.OLLAMA_API_KEY || 'optional'}`
    },
    body: JSON.stringify({
      userInput: userInput,
      musicDNA: musicDNA,
      context: context,
      max_tokens: 1000,
      temperature: 0.8
    })
  });

  if (!response.ok) {
    throw new Error(`Direct Ollama API error: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  console.log('ðŸ¤– Direct Ollama response received');

  return {
    success: true,
    code: data.code,
    description: data.description,
    metadata: {
      ...data.metadata,
      ai_source: 'ollama_deepseek_r1',
      raw_response: JSON.stringify(data).substring(0, 200) + '...'
    },
    uniqueness: 0.9,
    analysis: {
      confidence: 0.9,
      innovation: 0.85
    }
  };
}

function parseOllamaResponse(runpodData, userInput, musicDNA) {
  let responseData = {};
  
  // Extract the response from RunPod wrapper
  if (runpodData.output) {
    responseData = runpodData.output;
  } else {
    throw new Error('No output in RunPod Ollama response');
  }

  console.log('ðŸ¤– Parsing Ollama response:', responseData);

  // Our Ollama handler returns structured data
  if (responseData.strudel_code && responseData.description) {
    return {
      success: true,
      code: responseData.strudel_code,
      description: responseData.description,
      metadata: {
        genre: musicDNA?.primaryGenre || extractGenre(userInput),
        mood: musicDNA?.preferredMood || 'creative',
        ai_source: 'ollama_deepseek_r1_runpod',
        ...responseData.metadata,
        raw_response: JSON.stringify(responseData).substring(0, 200) + '...'
      },
      uniqueness: 0.9,
      analysis: {
        confidence: 0.9,
        innovation: 0.85
      }
    };
  }

  // Fallback parsing for unexpected formats
  return generateFallbackFromAI(JSON.stringify(responseData), userInput, musicDNA);
}

// Updated main API endpoint with Ollama integration
app.post('/api/generate-music', async (req, res) => {
  try {
    const { userInput, musicDNA, context, requestPhase } = req.body;
    
    console.log('ðŸŽµ AI Request:', { userInput, musicDNA, context, requestPhase });
    
    // Try OpenAI GPT-4 API first (if available and enabled)
    if (OPENAI_API_KEY && OPENAI_API_KEY !== 'your_openai_key_here' && process.env.OPENAI_ENABLED === 'true') {
      console.log('ðŸ¤– Attempting OpenAI GPT-4 generation...');
      
      try {
        const openaiResult = await callOpenAIAPI(userInput, musicDNA, context);
        if (openaiResult.success) {
          res.json({
            success: true,
            code: openaiResult.code,
            description: openaiResult.description + ' (Generated by Nala AI - GPT-4 via OpenAI)',
            metadata: {
              ...openaiResult.metadata,
              timestamp: new Date().toISOString(),
              source: 'nala_gpt4_openai',
              uniqueness: openaiResult.uniqueness || 1.0,
              semantic_analysis: openaiResult.analysis
            }
          });
          return;
        }
      } catch (openaiError) {
        console.error('ðŸ¤– OpenAI API failed:', openaiError);
        console.log('ðŸ”„ Trying RunPod Ollama fallback...');
      }
    }
    
    // Try RunPod Ollama DeepSeek R1 API (NEW ENHANCED VERSION)
    if (RUNPOD_OLLAMA_ENDPOINT && process.env.RUNPOD_OLLAMA_ENABLED !== 'false') {
      console.log('ðŸ¤– Attempting RunPod Ollama DeepSeek R1 generation...');
      
      try {
        const ollamaResult = await callRunPodOllamaAPI(userInput, musicDNA, context);
        if (ollamaResult.success) {
          res.json({
            success: true,
            code: ollamaResult.code,
            description: ollamaResult.description + ' (Generated by Nala AI - DeepSeek R1 via RunPod Ollama)',
            metadata: {
              ...ollamaResult.metadata,
              timestamp: new Date().toISOString(),
              source: 'nala_ollama_deepseek_r1_runpod',
              uniqueness: ollamaResult.uniqueness || 1.0,
              semantic_analysis: ollamaResult.analysis
            }
          });
          return;
        }
      } catch (ollamaError) {
        console.error('ðŸ¤– RunPod Ollama API failed:', ollamaError);
        console.log('ðŸ”„ Trying direct Ollama fallback...');
        
        // Try direct Ollama endpoint as backup
        if (process.env.OLLAMA_ENDPOINT) {
          try {
            const directOllamaResult = await callDirectOllamaAPI(userInput, musicDNA, context);
            if (directOllamaResult.success) {
              res.json({
                success: true,
                code: directOllamaResult.code,
                description: directOllamaResult.description + ' (Generated by Nala AI - Direct Ollama)',
                metadata: {
                  ...directOllamaResult.metadata,
                  timestamp: new Date().toISOString(),
                  source: 'nala_direct_ollama',
                  uniqueness: directOllamaResult.uniqueness || 1.0,
                  semantic_analysis: directOllamaResult.analysis
                }
              });
              return;
            }
          } catch (directOllamaError) {
            console.error('ðŸ¤– Direct Ollama API failed:', directOllamaError);
            console.log('ðŸ”„ Falling back to original RunPod...');
          }
        }
      }
    }
    
    // Try original RunPod DeepSeek R1 API (if available)
    if (RUNPOD_API_KEY && RUNPOD_API_KEY !== 'YOUR_API_KEY' && RUNPOD_API_KEY !== 'disabled') {
      console.log('ðŸ¤– Attempting original RunPod DeepSeek R1 generation...');
      
      try {
        const runpodResult = await callRunPodAPI(userInput, musicDNA, context);
        if (runpodResult.success) {
          res.json({
            success: true,
            code: runpodResult.code,
            description: runpodResult.description + ' (Generated by Nala AI - DeepSeek R1 via RunPod)',
            metadata: {
              ...runpodResult.metadata,
              timestamp: new Date().toISOString(),
              source: 'nala_deepseek_r1_runpod',
              uniqueness: runpodResult.uniqueness || 1.0,
              semantic_analysis: runpodResult.analysis
            }
          });
          return;
        }
      } catch (runpodError) {
        console.error('ðŸ¤– Original RunPod API failed:', runpodError);
        console.log('ðŸ”„ Falling back to local AI systems...');
      }
    }
    
    // Continue with existing Phase 3, Phase 2, Phase 1 fallbacks...
    // (rest of the existing fallback logic remains the same)
    
  } catch (error) {
    console.error('AI Generation Error:', error);
    
    // Fallback to enhanced AI-like generation
    const fallbackResult = generateFallbackPattern(req.body.userInput);
    
    res.json({
      success: true,
      code: fallbackResult.code,
      description: fallbackResult.description + ' (Generated by Nala AI - Enhanced Mode)',
      error: error.message,
      metadata: {
        genre: extractGenre(req.body.userInput),
        timestamp: new Date().toISOString(),
        source: 'nala_enhanced'
      }
    });
  }
});

// Updated health check to include Ollama status
app.get('/api/health', (req, res) => {
  const hasRunPodKey = RUNPOD_API_KEY && RUNPOD_API_KEY !== 'YOUR_API_KEY';
  const hasOllamaEndpoint = RUNPOD_OLLAMA_ENDPOINT && RUNPOD_OLLAMA_ENDPOINT.includes('runpod');
  
  res.json({ 
    status: 'ok', 
    timestamp: new Date().toISOString(),
    ai: 'multi-tier-with-ollama',
    service: 'nala-music-ai',
    runpod: {
      configured: hasRunPodKey,
      endpoint: hasRunPodKey ? RUNPOD_ENDPOINT : 'not configured'
    },
    ollama: {
      configured: hasOllamaEndpoint,
      endpoint: hasOllamaEndpoint ? RUNPOD_OLLAMA_ENDPOINT : 'not configured',
      enabled: process.env.RUNPOD_OLLAMA_ENABLED !== 'false'
    },
    features: {
      phase1: true,
      phase2: process.env.PHASE2_ENABLED === 'true',
      phase3: process.env.PHASE3_ENABLED === 'true',
      runpod_ai: hasRunPodKey,
      ollama_ai: hasOllamaEndpoint,
      openai_ai: OPENAI_API_KEY && OPENAI_API_KEY !== 'your_openai_key_here'
    }
  });
});

module.exports = {
  callRunPodOllamaAPI,
  callDirectOllamaAPI,
  parseOllamaResponse
};